{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghosh-sarbajit/AboutNLP/blob/main/Multilinear_Perceptron_for_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Citation](https://web.stanford.edu/class/cs224n/)**\n",
        "\n",
        "* [\"Word Window Classification\" tutorial notebook]((https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/materials/ww_classifier.ipynb) by Matt Lamm, from Winter 2020 offering of CS224N\n",
        "* Official PyTorch Documentation on [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) by Soumith Chintala\n",
        "* PyTorch Tutorial Notebook, [Build Basic Generative Adversarial Networks (GANs) | Coursera](https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans) by Sharon Zhou, offered on Coursera\n",
        "\n",
        "* [PyTorch](https://pytorch.org/)\n",
        "* [TensorFlow](https://www.tensorflow.org/)\n",
        "* [Installation](https://pytorch.org/)"
      ],
      "metadata": {
        "id": "3wd9xUEU-2mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of zip function of python\n",
        "x = [1,2,3]\n",
        "y = [4,5,6]\n",
        "zipped = zip(x,y)\n",
        "zipped_list=list(zipped)\n",
        "\n",
        "print(x, \"\\t\", y, \"\\t\", *zipped)\n",
        "\n",
        "print(x, \"\\t\", y, \"\\t\", zipped_list)\n",
        "\n",
        "x1, y1 = zip(*zipped_list)\n",
        "\n",
        "print(x1, \"\\t\", y1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km_aIA3dAsTb",
        "outputId": "18e59432-7643-4d95-9146-216df3212b35"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3] \t [4, 5, 6] \t\n",
            "[1, 2, 3] \t [4, 5, 6] \t [(1, 4), (2, 5), (3, 6)]\n",
            "(1, 2, 3) \t (4, 5, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from functools import partial # Je kono function er structure niye onyo kaje babohar kore (fol mix er kaj thakle sobji mix kora jai)\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "JY_waYqkZXWD"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "Tn8hGbrZRfmz"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "          \"We always come to Paris\",\n",
        "          \"The professor is from Australia\",\n",
        "          \"I live in Stanford\",\n",
        "          \"He comes from Taiwan\",\n",
        "          \"The capital of Turkey is Ankara or Paris\"\n",
        "         ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The preprocessing function we will use to generate our training examples\n",
        "# Our function is a simple one, we lowercase the letters\n",
        "# and then tokenize the words.\n",
        "def preprocess_sentence(sentence):\n",
        "  return sentence.lower().split()\n",
        "\n",
        "# Create our training set\n",
        "train_sentences = [preprocess_sentence(sent) for sent in corpus]\n",
        "print(train_sentences)\n",
        "\n",
        "# Set of locations that appear in our corpus\n",
        "locations = set([\"australia\", \"ankara\", \"paris\", \"stanford\", \"taiwan\", \"turkey\"])\n",
        "\n",
        "# Our train labels\n",
        "train_labels = [[1 if word in locations else 0 for word in sent] for sent in train_sentences]\n",
        "print(train_labels)\n",
        "\n",
        "# Find all the unique words in our corpus\n",
        "vocabulary = set(w for s in train_sentences for w in s)\n",
        "\n",
        "# add <unk> and <pad> tokens\n",
        "vocabulary.add(\"<unk>\")\n",
        "vocabulary.add(\"<pad>\")\n",
        "\n",
        "ix_to_word = sorted(list(vocabulary))\n",
        "word_to_ix = {word: ind for ind, word in enumerate(ix_to_word)}"
      ],
      "metadata": {
        "id": "SiMgcPCDcf3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7432571-93c4-4168-af3e-2bfe37f28c35"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['we', 'always', 'come', 'to', 'paris'], ['the', 'professor', 'is', 'from', 'australia'], ['i', 'live', 'in', 'stanford'], ['he', 'comes', 'from', 'taiwan'], ['the', 'capital', 'of', 'turkey', 'is', 'ankara', 'or', 'paris']]\n",
            "[[0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1, 0, 1, 0, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch, window_size, word_to_ix):\n",
        "  # Let training examples (x) and labels (y)\n",
        "  # Break our batch into the training examples (x) and labels (y)\n",
        "  # We are turning our x and y into tensors because nn.utils.rnn.pad_sequence method expects tensors.\n",
        "  # This is also useful since our model will be expecting tensor inputs.\n",
        "  x, y = zip(*batch)\n",
        "\n",
        "  # Now we need to window pad our training examples.\n",
        "  # We have already defined a function to handle window padding.\n",
        "  # We are including it here again so that everything is in one place.\n",
        "  def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
        "    window = [pad_token] * window_size\n",
        "    return window + sentence + window\n",
        "\n",
        "  # Pad the train examples.\n",
        "  x = [pad_window(s, window_size=window_size) for s in x]\n",
        "\n",
        "  # Now we need to turn words in our training examples to indices.\n",
        "  # We are copying the function defined earlier for the same reason as above.\n",
        "  def convert_tokens_to_indices(sentence, word_to_ix):\n",
        "    return [word_to_ix.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
        "\n",
        "  # Convert the train examples into indices.\n",
        "  x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n",
        "\n",
        "  # We will now pad the examples so that the lengths of all the example in one batch are the same, making it possible to do matrix operations.\n",
        "  # We set the batch_first parameter to True so that the returned matrix has the batch as the first dimension.\n",
        "  pad_token_ix = word_to_ix[\"<pad>\"]\n",
        "\n",
        "  # pad_sequence function expects the input to be a tensor, so we turn x into one\n",
        "  x = [torch.LongTensor(x_i) for x_i in x]\n",
        "  x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
        "\n",
        "  # We will also pad the labels.\n",
        "  # Before doing so, we will record the number of labels so that we know how many words existed in each example.\n",
        "  lengths = [len(label) for label in y]\n",
        "  lenghts = torch.LongTensor(lengths)\n",
        "\n",
        "  y = [torch.LongTensor(y_i) for y_i in y]\n",
        "  y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
        "\n",
        "  # We are now ready to return our variables. The order we return our variables\n",
        "  # here will match the order we read them in our training loop.\n",
        "  return x_padded, y_padded, lenghts"
      ],
      "metadata": {
        "id": "VqMT0P3wUvtu"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ei block ta dataloader kmn bhabe kaj korche seta bojhanor jonyo jodio main code e etar kono bhumika nei\n",
        "# Parameters to be passed to the DataLoader\n",
        "data = list(zip(train_sentences, train_labels))\n",
        "print(data, \"\\n\")\n",
        "\n",
        "batch_size = 2\n",
        "shuffle = False\n",
        "window_size = 2\n",
        "\n",
        "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix) # word_to_index\n",
        "print(collate_fn, \"\\n\")\n",
        "\n",
        "# Instantiate the DataLoader\n",
        "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
        "\n",
        "print(loader, \"\\n\")\n",
        "\n",
        "# Go through one loop\n",
        "counter = 0\n",
        "for batched_x, batched_y, batched_lengths in loader:\n",
        "  print(f\"Iteration {counter}\")\n",
        "  print(\"Batched Input:\")\n",
        "  print(batched_x)\n",
        "  print(\"Batched Labels:\")\n",
        "  print(batched_y)\n",
        "  print(\"Batched Lengths:\")\n",
        "  print(batched_lengths)\n",
        "  print(\"\")\n",
        "  counter += 1\n",
        "\n",
        "# corpus = [\n",
        "#           \"We always come to Paris\",\n",
        "#           \"The professor is from Australia\",\n",
        "#           \"I live in Stanford\",\n",
        "#           \"He comes from Taiwan\",\n",
        "#           \"The capital of Turkey is Ankara or Paris\"\n",
        "#          ]\n",
        "# [[23, 2, 6, 21, 16],\n",
        "#  [20, 17, 12, 8, 4],\n",
        "#  [10, 13, 11, 18],\n",
        "#  [9, 7, 8, 19],\n",
        "#  [20, 5, 14, 22, 12, 3, 15, 16]]\n",
        "\n",
        "\n",
        "# Print the original tensor\n",
        "print(f\"Original Tensor: \")\n",
        "print(batched_x)\n",
        "print(\"\")\n",
        "\n",
        "# Create the 2 * 2 + 1 chunks\n",
        "chunk = batched_x.unfold(1, window_size*2 + 1, 1)\n",
        "print(f\"Windows: \")\n",
        "print(chunk)"
      ],
      "metadata": {
        "id": "XOm2m7PMVUgV",
        "outputId": "eb2a86f1-af71-47d5-8bb3-2209a3bf198e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['we', 'always', 'come', 'to', 'paris'], [0, 0, 0, 0, 1]), (['the', 'professor', 'is', 'from', 'australia'], [0, 0, 0, 0, 1]), (['i', 'live', 'in', 'stanford'], [0, 0, 0, 1]), (['he', 'comes', 'from', 'taiwan'], [0, 0, 0, 1]), (['the', 'capital', 'of', 'turkey', 'is', 'ankara', 'or', 'paris'], [0, 0, 0, 1, 0, 1, 0, 1])] \n",
            "\n",
            "functools.partial(<function custom_collate_fn at 0x7823bda7b2e0>, window_size=2, word_to_ix={'<pad>': 0, '<unk>': 1, 'always': 2, 'ankara': 3, 'australia': 4, 'capital': 5, 'come': 6, 'comes': 7, 'from': 8, 'he': 9, 'i': 10, 'in': 11, 'is': 12, 'live': 13, 'of': 14, 'or': 15, 'paris': 16, 'professor': 17, 'stanford': 18, 'taiwan': 19, 'the': 20, 'to': 21, 'turkey': 22, 'we': 23}) \n",
            "\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7823bdb7cd90> \n",
            "\n",
            "Iteration 0\n",
            "Batched Input:\n",
            "tensor([[ 0,  0, 23,  2,  6, 21, 16,  0,  0],\n",
            "        [ 0,  0, 20, 17, 12,  8,  4,  0,  0]])\n",
            "Batched Labels:\n",
            "tensor([[0, 0, 0, 0, 1],\n",
            "        [0, 0, 0, 0, 1]])\n",
            "Batched Lengths:\n",
            "tensor([5, 5])\n",
            "\n",
            "Iteration 1\n",
            "Batched Input:\n",
            "tensor([[ 0,  0, 10, 13, 11, 18,  0,  0],\n",
            "        [ 0,  0,  9,  7,  8, 19,  0,  0]])\n",
            "Batched Labels:\n",
            "tensor([[0, 0, 0, 1],\n",
            "        [0, 0, 0, 1]])\n",
            "Batched Lengths:\n",
            "tensor([4, 4])\n",
            "\n",
            "Iteration 2\n",
            "Batched Input:\n",
            "tensor([[ 0,  0, 20,  5, 14, 22, 12,  3, 15, 16,  0,  0]])\n",
            "Batched Labels:\n",
            "tensor([[0, 0, 0, 1, 0, 1, 0, 1]])\n",
            "Batched Lengths:\n",
            "tensor([8])\n",
            "\n",
            "Original Tensor: \n",
            "tensor([[ 0,  0, 20,  5, 14, 22, 12,  3, 15, 16,  0,  0]])\n",
            "\n",
            "Windows: \n",
            "tensor([[[ 0,  0, 20,  5, 14],\n",
            "         [ 0, 20,  5, 14, 22],\n",
            "         [20,  5, 14, 22, 12],\n",
            "         [ 5, 14, 22, 12,  3],\n",
            "         [14, 22, 12,  3, 15],\n",
            "         [22, 12,  3, 15, 16],\n",
            "         [12,  3, 15, 16,  0],\n",
            "         [ 3, 15, 16,  0,  0]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WordWindowClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, hyperparameters, vocab_size, pad_ix=0):\n",
        "    super(WordWindowClassifier, self).__init__()\n",
        "\n",
        "    \"\"\" Instance variables \"\"\"\n",
        "    self.window_size = hyperparameters[\"window_size\"]\n",
        "    self.embed_dim = hyperparameters[\"embed_dim\"]\n",
        "    self.hidden_dim = hyperparameters[\"hidden_dim\"]\n",
        "    self.freeze_embeddings = hyperparameters[\"freeze_embeddings\"]\n",
        "\n",
        "    \"\"\" Embedding Layer\n",
        "    Takes in a tensor containing embedding indices, and returns the\n",
        "    corresponding embeddings. The output is of dim\n",
        "    (number_of_indices * embedding_dim).\n",
        "\n",
        "    If freeze_embeddings is True, set the embedding layer parameters to be\n",
        "    non-trainable. This is useful if we only want the parameters other than the\n",
        "    embeddings parameters to change.\n",
        "    \"\"\"\n",
        "\n",
        "    self.embeds = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_ix)\n",
        "    if self.freeze_embeddings:\n",
        "      self.embed_layer.weight.requires_grad = False\n",
        "\n",
        "    \"\"\" Hidden Layer\n",
        "    \"\"\"\n",
        "    full_window_size = 2 * window_size + 1\n",
        "    self.hidden_layer = nn.Sequential(\n",
        "      nn.Linear(full_window_size * self.embed_dim, self.hidden_dim),\n",
        "      nn.Tanh()\n",
        "    )\n",
        "\n",
        "    \"\"\" Output Layer\n",
        "    \"\"\"\n",
        "    self.output_layer = nn.Linear(self.hidden_dim, 1)\n",
        "\n",
        "    \"\"\" Probabilities\n",
        "    \"\"\"\n",
        "    self.probabilities = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    \"\"\"\n",
        "    Let B:= batch_size\n",
        "        L:= window-padded sentence length\n",
        "        D:= self.embed_dim\n",
        "        S:= self.window_size\n",
        "        H:= self.hidden_dim\n",
        "\n",
        "    inputs: a (B, L) tensor of token indices\n",
        "    \"\"\"\n",
        "    B, L = inputs.size()\n",
        "\n",
        "    \"\"\"\n",
        "    Reshaping.\n",
        "    Takes in a (B, L) LongTensor\n",
        "    Outputs a (B, L~, S) LongTensor\n",
        "    \"\"\"\n",
        "    # Fist, get our word windows for each word in our input.\n",
        "    token_windows = inputs.unfold(1, 2 * self.window_size + 1, 1)\n",
        "    _, adjusted_length, _ = token_windows.size()\n",
        "\n",
        "    # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
        "    assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)\n",
        "\n",
        "    \"\"\"\n",
        "    Embedding.\n",
        "    Takes in a torch.LongTensor of size (B, L~, S)\n",
        "    Outputs a (B, L~, S, D) FloatTensor.\n",
        "    \"\"\"\n",
        "    embedded_windows = self.embeds(token_windows)\n",
        "\n",
        "    \"\"\"\n",
        "    Reshaping.\n",
        "    Takes in a (B, L~, S, D) FloatTensor.\n",
        "    Resizes it into a (B, L~, S*D) FloatTensor.\n",
        "    -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
        "    \"\"\"\n",
        "    embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
        "\n",
        "    \"\"\"\n",
        "    Layer 1.\n",
        "    Takes in a (B, L~, S*D) FloatTensor.\n",
        "    Resizes it into a (B, L~, H) FloatTensor\n",
        "    \"\"\"\n",
        "    layer_1 = self.hidden_layer(embedded_windows)\n",
        "\n",
        "    \"\"\"\n",
        "    Layer 2\n",
        "    Takes in a (B, L~, H) FloatTensor.\n",
        "    Resizes it into a (B, L~, 1) FloatTensor.\n",
        "    \"\"\"\n",
        "    output = self.output_layer(layer_1)\n",
        "\n",
        "    \"\"\"\n",
        "    Softmax.\n",
        "    Takes in a (B, L~, 1) FloatTensor of unnormalized class scores.\n",
        "    Outputs a (B, L~, 1) FloatTensor of (log-)normalized class scores.\n",
        "    \"\"\"\n",
        "    output = self.probabilities(output)\n",
        "    output = output.view(B, -1)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "8CAx5G78VXw3"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data\n",
        "data = list(zip(train_sentences, train_labels))\n",
        "batch_size = 2\n",
        "shuffle = True\n",
        "window_size = 2\n",
        "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
        "\n",
        "# Instantiate a DataLoader\n",
        "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize a model\n",
        "# It is useful to put all the model hyperparameters in a dictionary\n",
        "model_hyperparameters = {\n",
        "    \"batch_size\": 4,\n",
        "    \"window_size\": 2,\n",
        "    \"embed_dim\": 25,\n",
        "    \"hidden_dim\": 25,\n",
        "    \"freeze_embeddings\": False,\n",
        "}\n",
        "\n",
        "vocab_size = len(word_to_ix)\n",
        "print(vocab_size)\n",
        "model = WordWindowClassifier(model_hyperparameters, vocab_size)\n",
        "\n",
        "# Define an optimizer\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define a loss function, which computes to binary cross entropy loss\n",
        "def loss_function(batch_outputs, batch_labels, batch_lengths):\n",
        "    # Calculate the loss for the whole batch\n",
        "    bceloss = nn.BCELoss() # Binary Cross Entropy Loss\n",
        "    loss = bceloss(batch_outputs, batch_labels.float())\n",
        "\n",
        "    # Rescale the loss. Remember that we have used lengths to store the\n",
        "    # number of words in each training example\n",
        "    loss = loss / batch_lengths.sum().float()\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Utrjc6BHVX0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6afb2efa-734f-4777-94ad-47febbf0f8c5"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that will be called in every epoch\n",
        "def train_epoch(loss_function, optimizer, model, loader):\n",
        "\n",
        "  # Keep track of the total loss for the batch\n",
        "  total_loss = 0\n",
        "  for batch_inputs, batch_labels, batch_lengths in loader:\n",
        "    # Clear the gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Run a forward pass\n",
        "    outputs = model.forward(batch_inputs)\n",
        "    # Compute the batch loss\n",
        "    loss = loss_function(outputs, batch_labels, batch_lengths)\n",
        "    # Calculate the gradients\n",
        "    loss.backward()\n",
        "    # Update the parameteres\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss, outputs\n",
        "\n",
        "\n",
        "# Function containing our main training loop\n",
        "def train(loss_function, optimizer, model, loader, num_epochs=10000):\n",
        "\n",
        "  # Iterate through each epoch and call our train_epoch function\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss, outputs = train_epoch(loss_function, optimizer, model, loader)\n",
        "    if epoch % 100 == 0: print(epoch_loss)\n",
        "\n",
        "  return outputs"
      ],
      "metadata": {
        "id": "qxuKVo_4VX6f"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1000\n",
        "outputs = train(loss_function, optimizer, model, loader, num_epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E74cKMdGVfzc",
        "outputId": "95d5a26b-62f7-4c2a-8ace-18972e07904b"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2373027577996254\n",
            "0.23785117641091347\n",
            "0.16980697959661484\n",
            "0.1298033818602562\n",
            "0.10767391137778759\n",
            "0.08576141111552715\n",
            "0.08239723555743694\n",
            "0.05538809206336737\n",
            "0.043667892925441265\n",
            "0.03852299600839615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6KHfC7tM0V0",
        "outputId": "16a1b8b5-e7d9-4670-9e8f-f2a261a488bf"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1261, 0.0567, 0.0521, 0.8970]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "iugrlebcGims"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "from IPython.display import Image, SVG, display\n",
        "\n",
        "dot = make_dot(outputs, params=dict(model.named_parameters()))\n",
        "dot.format = 'svg'\n",
        "# dot.render('WordWindowClassifier')\n",
        "dot.render(\"word_window_classifier\")\n",
        "# display(Image(\"WordWindowClassifier.jpg\"))\n",
        "display(SVG(\"word_window_classifier.svg\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q80rYxxpGlcJ",
        "outputId": "d86ba92b-b0ef-4d9e-863d-e66fbccb633a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"419pt\" height=\"821pt\" viewBox=\"0.00 0.00 419.00 821.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 817)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-817 415,-817 415,4 -4,4\"/>\n<!-- 132094899980864 -->\n<g id=\"node1\" class=\"node\">\n<title>132094899980864</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"230,-31 171,-31 171,0 230,0 230,-31\"/>\n<text text-anchor=\"middle\" x=\"200.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (1, 4)</text>\n</g>\n<!-- 132094902129648 -->\n<g id=\"node2\" class=\"node\">\n<title>132094902129648</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"196,-92 101,-92 101,-73 196,-73 196,-92\"/>\n<text text-anchor=\"middle\" x=\"148.5\" y=\"-80\" font-family=\"monospace\" font-size=\"10.00\">ViewBackward0</text>\n</g>\n<!-- 132094902129648&#45;&gt;132094899980864 -->\n<g id=\"edge23\" class=\"edge\">\n<title>132094902129648-&gt;132094899980864</title>\n<path fill=\"none\" stroke=\"black\" d=\"M155.51,-72.73C162.38,-64.15 173.14,-50.69 182.42,-39.1\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"185.32,-41.08 188.83,-31.08 179.85,-36.71 185.32,-41.08\"/>\n</g>\n<!-- 132094902127824 -->\n<g id=\"node3\" class=\"node\">\n<title>132094902127824</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"257,-153 144,-153 144,-134 257,-134 257,-153\"/>\n<text text-anchor=\"middle\" x=\"200.5\" y=\"-141\" font-family=\"monospace\" font-size=\"10.00\">SigmoidBackward0</text>\n</g>\n<!-- 132094902127824&#45;&gt;132094902129648 -->\n<g id=\"edge1\" class=\"edge\">\n<title>132094902127824-&gt;132094902129648</title>\n<path fill=\"none\" stroke=\"black\" d=\"M192.83,-133.79C184.99,-124.91 172.64,-110.89 162.96,-99.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"165.44,-97.43 156.2,-92.24 160.19,-102.06 165.44,-97.43\"/>\n</g>\n<!-- 132094899980464 -->\n<g id=\"node25\" class=\"node\">\n<title>132094899980464</title>\n<polygon fill=\"#a2cd5a\" stroke=\"black\" points=\"291,-98 214,-98 214,-67 291,-67 291,-98\"/>\n<text text-anchor=\"middle\" x=\"252.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\"> (1, 4, 1)</text>\n</g>\n<!-- 132094902127824&#45;&gt;132094899980464 -->\n<g id=\"edge24\" class=\"edge\">\n<title>132094902127824-&gt;132094899980464</title>\n<path fill=\"none\" stroke=\"black\" d=\"M208.17,-133.79C214.68,-126.41 224.31,-115.49 232.93,-105.7\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"235.56,-108.01 239.55,-98.19 230.31,-103.38 235.56,-108.01\"/>\n</g>\n<!-- 132094902134208 -->\n<g id=\"node4\" class=\"node\">\n<title>132094902134208</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"248,-208 153,-208 153,-189 248,-189 248,-208\"/>\n<text text-anchor=\"middle\" x=\"200.5\" y=\"-196\" font-family=\"monospace\" font-size=\"10.00\">ViewBackward0</text>\n</g>\n<!-- 132094902134208&#45;&gt;132094902127824 -->\n<g id=\"edge2\" class=\"edge\">\n<title>132094902134208-&gt;132094902127824</title>\n<path fill=\"none\" stroke=\"black\" d=\"M200.5,-188.75C200.5,-181.8 200.5,-171.85 200.5,-163.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"204,-163.09 200.5,-153.09 197,-163.09 204,-163.09\"/>\n</g>\n<!-- 132094902134448 -->\n<g id=\"node5\" class=\"node\">\n<title>132094902134448</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"251,-263 150,-263 150,-244 251,-244 251,-263\"/>\n<text text-anchor=\"middle\" x=\"200.5\" y=\"-251\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 132094902134448&#45;&gt;132094902134208 -->\n<g id=\"edge3\" class=\"edge\">\n<title>132094902134448-&gt;132094902134208</title>\n<path fill=\"none\" stroke=\"black\" d=\"M200.5,-243.75C200.5,-236.8 200.5,-226.85 200.5,-218.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"204,-218.09 200.5,-208.09 197,-218.09 204,-218.09\"/>\n</g>\n<!-- 132094902140112 -->\n<g id=\"node6\" class=\"node\">\n<title>132094902140112</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"130,-318 29,-318 29,-299 130,-299 130,-318\"/>\n<text text-anchor=\"middle\" x=\"79.5\" y=\"-306\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 132094902140112&#45;&gt;132094902134448 -->\n<g id=\"edge4\" class=\"edge\">\n<title>132094902140112-&gt;132094902134448</title>\n<path fill=\"none\" stroke=\"black\" d=\"M98.94,-298.98C118.49,-290.42 148.88,-277.11 171.32,-267.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"172.95,-270.39 180.71,-263.17 170.14,-263.98 172.95,-270.39\"/>\n</g>\n<!-- 132094922830960 -->\n<g id=\"node7\" class=\"node\">\n<title>132094922830960</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"135,-384 16,-384 16,-354 135,-354 135,-384\"/>\n<text text-anchor=\"middle\" x=\"75.5\" y=\"-372\" font-family=\"monospace\" font-size=\"10.00\">output_layer.bias</text>\n<text text-anchor=\"middle\" x=\"75.5\" y=\"-361\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n</g>\n<!-- 132094922830960&#45;&gt;132094902140112 -->\n<g id=\"edge5\" class=\"edge\">\n<title>132094922830960-&gt;132094902140112</title>\n<path fill=\"none\" stroke=\"black\" d=\"M76.47,-353.84C76.99,-346.21 77.64,-336.7 78.2,-328.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"81.71,-328.48 78.9,-318.27 74.73,-328 81.71,-328.48\"/>\n</g>\n<!-- 132094902127104 -->\n<g id=\"node8\" class=\"node\">\n<title>132094902127104</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"248,-318 153,-318 153,-299 248,-299 248,-318\"/>\n<text text-anchor=\"middle\" x=\"200.5\" y=\"-306\" font-family=\"monospace\" font-size=\"10.00\">ViewBackward0</text>\n</g>\n<!-- 132094902127104&#45;&gt;132094902134448 -->\n<g id=\"edge6\" class=\"edge\">\n<title>132094902127104-&gt;132094902134448</title>\n<path fill=\"none\" stroke=\"black\" d=\"M200.5,-298.75C200.5,-291.8 200.5,-281.85 200.5,-273.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"204,-273.09 200.5,-263.09 197,-273.09 204,-273.09\"/>\n</g>\n<!-- 132094902126240 -->\n<g id=\"node9\" class=\"node\">\n<title>132094902126240</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"248,-378.5 153,-378.5 153,-359.5 248,-359.5 248,-378.5\"/>\n<text text-anchor=\"middle\" x=\"200.5\" y=\"-366.5\" font-family=\"monospace\" font-size=\"10.00\">TanhBackward0</text>\n</g>\n<!-- 132094902126240&#45;&gt;132094902127104 -->\n<g id=\"edge7\" class=\"edge\">\n<title>132094902126240-&gt;132094902127104</title>\n<path fill=\"none\" stroke=\"black\" d=\"M200.5,-359.37C200.5,-351.25 200.5,-338.81 200.5,-328.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"204,-328.17 200.5,-318.17 197,-328.17 204,-328.17\"/>\n</g>\n<!-- 132094902126480 -->\n<g id=\"node10\" class=\"node\">\n<title>132094902126480</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"244,-444.5 149,-444.5 149,-425.5 244,-425.5 244,-444.5\"/>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-432.5\" font-family=\"monospace\" font-size=\"10.00\">ViewBackward0</text>\n</g>\n<!-- 132094902126480&#45;&gt;132094902126240 -->\n<g id=\"edge8\" class=\"edge\">\n<title>132094902126480-&gt;132094902126240</title>\n<path fill=\"none\" stroke=\"black\" d=\"M197.04,-425.37C197.62,-416.07 198.56,-400.98 199.32,-388.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"202.81,-389.1 199.94,-378.91 195.83,-388.67 202.81,-389.1\"/>\n</g>\n<!-- 132094902133200 -->\n<g id=\"node11\" class=\"node\">\n<title>132094902133200</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"247,-505 146,-505 146,-486 247,-486 247,-505\"/>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-493\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 132094902133200&#45;&gt;132094902126480 -->\n<g id=\"edge9\" class=\"edge\">\n<title>132094902133200-&gt;132094902126480</title>\n<path fill=\"none\" stroke=\"black\" d=\"M196.5,-485.87C196.5,-477.75 196.5,-465.31 196.5,-454.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"200,-454.67 196.5,-444.67 193,-454.67 200,-454.67\"/>\n</g>\n<!-- 132094902128880 -->\n<g id=\"node12\" class=\"node\">\n<title>132094902128880</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"124,-560 23,-560 23,-541 124,-541 124,-560\"/>\n<text text-anchor=\"middle\" x=\"73.5\" y=\"-548\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 132094902128880&#45;&gt;132094902133200 -->\n<g id=\"edge10\" class=\"edge\">\n<title>132094902128880-&gt;132094902133200</title>\n<path fill=\"none\" stroke=\"black\" d=\"M93.26,-540.98C113.23,-532.38 144.3,-518.99 167.15,-509.15\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"168.58,-512.34 176.38,-505.17 165.81,-505.91 168.58,-512.34\"/>\n</g>\n<!-- 132094899987664 -->\n<g id=\"node13\" class=\"node\">\n<title>132094899987664</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"131,-626 0,-626 0,-596 131,-596 131,-626\"/>\n<text text-anchor=\"middle\" x=\"65.5\" y=\"-614\" font-family=\"monospace\" font-size=\"10.00\">hidden_layer.0.bias</text>\n<text text-anchor=\"middle\" x=\"65.5\" y=\"-603\" font-family=\"monospace\" font-size=\"10.00\"> (25)</text>\n</g>\n<!-- 132094899987664&#45;&gt;132094902128880 -->\n<g id=\"edge11\" class=\"edge\">\n<title>132094899987664-&gt;132094902128880</title>\n<path fill=\"none\" stroke=\"black\" d=\"M67.44,-595.84C68.48,-588.21 69.78,-578.7 70.91,-570.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"74.41,-570.65 72.3,-560.27 67.48,-569.7 74.41,-570.65\"/>\n</g>\n<!-- 132094902128832 -->\n<g id=\"node14\" class=\"node\">\n<title>132094902128832</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"244,-560 149,-560 149,-541 244,-541 244,-560\"/>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-548\" font-family=\"monospace\" font-size=\"10.00\">ViewBackward0</text>\n</g>\n<!-- 132094902128832&#45;&gt;132094902133200 -->\n<g id=\"edge12\" class=\"edge\">\n<title>132094902128832-&gt;132094902133200</title>\n<path fill=\"none\" stroke=\"black\" d=\"M196.5,-540.75C196.5,-533.8 196.5,-523.85 196.5,-515.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"200,-515.09 196.5,-505.09 193,-515.09 200,-515.09\"/>\n</g>\n<!-- 132094902128928 -->\n<g id=\"node15\" class=\"node\">\n<title>132094902128928</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"244,-620.5 149,-620.5 149,-601.5 244,-601.5 244,-620.5\"/>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-608.5\" font-family=\"monospace\" font-size=\"10.00\">ViewBackward0</text>\n</g>\n<!-- 132094902128928&#45;&gt;132094902128832 -->\n<g id=\"edge13\" class=\"edge\">\n<title>132094902128928-&gt;132094902128832</title>\n<path fill=\"none\" stroke=\"black\" d=\"M196.5,-601.37C196.5,-593.25 196.5,-580.81 196.5,-570.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"200,-570.17 196.5,-560.17 193,-570.17 200,-570.17\"/>\n</g>\n<!-- 132094902132528 -->\n<g id=\"node16\" class=\"node\">\n<title>132094902132528</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"250,-686.5 125,-686.5 125,-667.5 250,-667.5 250,-686.5\"/>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-674.5\" font-family=\"monospace\" font-size=\"10.00\">EmbeddingBackward0</text>\n</g>\n<!-- 132094902132528&#45;&gt;132094902128928 -->\n<g id=\"edge14\" class=\"edge\">\n<title>132094902132528-&gt;132094902128928</title>\n<path fill=\"none\" stroke=\"black\" d=\"M188.71,-667.37C190.02,-658.07 192.14,-642.98 193.84,-630.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"197.32,-631.3 195.25,-620.91 190.39,-630.32 197.32,-631.3\"/>\n</g>\n<!-- 132094902139392 -->\n<g id=\"node17\" class=\"node\">\n<title>132094902139392</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"238,-747 137,-747 137,-728 238,-728 238,-747\"/>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-735\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 132094902139392&#45;&gt;132094902132528 -->\n<g id=\"edge15\" class=\"edge\">\n<title>132094902139392-&gt;132094902132528</title>\n<path fill=\"none\" stroke=\"black\" d=\"M187.5,-727.87C187.5,-719.75 187.5,-707.31 187.5,-696.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"191,-696.67 187.5,-686.67 184,-696.67 191,-696.67\"/>\n</g>\n<!-- 132094900687616 -->\n<g id=\"node18\" class=\"node\">\n<title>132094900687616</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"235,-813 140,-813 140,-783 235,-783 235,-813\"/>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-801\" font-family=\"monospace\" font-size=\"10.00\">embeds.weight</text>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-790\" font-family=\"monospace\" font-size=\"10.00\"> (24, 25)</text>\n</g>\n<!-- 132094900687616&#45;&gt;132094902139392 -->\n<g id=\"edge16\" class=\"edge\">\n<title>132094900687616-&gt;132094902139392</title>\n<path fill=\"none\" stroke=\"black\" d=\"M187.5,-782.84C187.5,-775.21 187.5,-765.7 187.5,-757.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"191,-757.27 187.5,-747.27 184,-757.27 191,-757.27\"/>\n</g>\n<!-- 132094902139536 -->\n<g id=\"node19\" class=\"node\">\n<title>132094902139536</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"354,-560 277,-560 277,-541 354,-541 354,-560\"/>\n<text text-anchor=\"middle\" x=\"315.5\" y=\"-548\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 132094902139536&#45;&gt;132094902133200 -->\n<g id=\"edge17\" class=\"edge\">\n<title>132094902139536-&gt;132094902133200</title>\n<path fill=\"none\" stroke=\"black\" d=\"M296.38,-540.98C277.15,-532.42 247.27,-519.11 225.19,-509.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"226.53,-506.04 215.97,-505.17 223.68,-512.44 226.53,-506.04\"/>\n</g>\n<!-- 132094902135696 -->\n<g id=\"node20\" class=\"node\">\n<title>132094902135696</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"381,-620.5 280,-620.5 280,-601.5 381,-601.5 381,-620.5\"/>\n<text text-anchor=\"middle\" x=\"330.5\" y=\"-608.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 132094902135696&#45;&gt;132094902139536 -->\n<g id=\"edge18\" class=\"edge\">\n<title>132094902135696-&gt;132094902139536</title>\n<path fill=\"none\" stroke=\"black\" d=\"M328.29,-601.37C326.18,-593.16 322.95,-580.54 320.26,-570.05\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"323.6,-568.98 317.72,-560.17 316.82,-570.72 323.6,-568.98\"/>\n</g>\n<!-- 132094900687056 -->\n<g id=\"node21\" class=\"node\">\n<title>132094900687056</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"411,-692 268,-692 268,-662 411,-662 411,-692\"/>\n<text text-anchor=\"middle\" x=\"339.5\" y=\"-680\" font-family=\"monospace\" font-size=\"10.00\">hidden_layer.0.weight</text>\n<text text-anchor=\"middle\" x=\"339.5\" y=\"-669\" font-family=\"monospace\" font-size=\"10.00\"> (25, 125)</text>\n</g>\n<!-- 132094900687056&#45;&gt;132094902135696 -->\n<g id=\"edge19\" class=\"edge\">\n<title>132094900687056-&gt;132094902135696</title>\n<path fill=\"none\" stroke=\"black\" d=\"M337.5,-661.8C336.22,-652.7 334.55,-640.79 333.16,-630.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"336.6,-630.26 331.74,-620.84 329.67,-631.23 336.6,-630.26\"/>\n</g>\n<!-- 132094902130944 -->\n<g id=\"node22\" class=\"node\">\n<title>132094902130944</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"349,-318 272,-318 272,-299 349,-299 349,-318\"/>\n<text text-anchor=\"middle\" x=\"310.5\" y=\"-306\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 132094902130944&#45;&gt;132094902134448 -->\n<g id=\"edge20\" class=\"edge\">\n<title>132094902130944-&gt;132094902134448</title>\n<path fill=\"none\" stroke=\"black\" d=\"M292.83,-298.98C275.21,-290.5 247.92,-277.35 227.57,-267.54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"229.02,-264.36 218.49,-263.17 225.98,-270.66 229.02,-264.36\"/>\n</g>\n<!-- 132094902137664 -->\n<g id=\"node23\" class=\"node\">\n<title>132094902137664</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"371,-378.5 270,-378.5 270,-359.5 371,-359.5 371,-378.5\"/>\n<text text-anchor=\"middle\" x=\"320.5\" y=\"-366.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 132094902137664&#45;&gt;132094902130944 -->\n<g id=\"edge21\" class=\"edge\">\n<title>132094902137664-&gt;132094902130944</title>\n<path fill=\"none\" stroke=\"black\" d=\"M319.02,-359.37C317.62,-351.16 315.46,-338.54 313.67,-328.05\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"317.12,-327.43 311.98,-318.17 310.22,-328.61 317.12,-327.43\"/>\n</g>\n<!-- 132094899861552 -->\n<g id=\"node24\" class=\"node\">\n<title>132094899861552</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"393,-450 262,-450 262,-420 393,-420 393,-450\"/>\n<text text-anchor=\"middle\" x=\"327.5\" y=\"-438\" font-family=\"monospace\" font-size=\"10.00\">output_layer.weight</text>\n<text text-anchor=\"middle\" x=\"327.5\" y=\"-427\" font-family=\"monospace\" font-size=\"10.00\"> (1, 25)</text>\n</g>\n<!-- 132094899861552&#45;&gt;132094902137664 -->\n<g id=\"edge22\" class=\"edge\">\n<title>132094899861552-&gt;132094902137664</title>\n<path fill=\"none\" stroke=\"black\" d=\"M325.95,-419.8C324.95,-410.7 323.65,-398.79 322.57,-388.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"326.03,-388.4 321.47,-378.84 319.08,-389.17 326.03,-388.4\"/>\n</g>\n<!-- 132094899980464&#45;&gt;132094899980864 -->\n<g id=\"edge25\" class=\"edge\">\n<title>132094899980464-&gt;132094899980864</title>\n<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M240.7,-66.75C234.18,-58.61 225.95,-48.32 218.64,-39.17\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"221.23,-36.81 212.25,-31.19 215.77,-41.19 221.23,-36.81\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test sentences\n",
        "test_corpus = [\"She went to Borojaguli via helabottola\"]\n",
        "test_sentences = [s.lower().split() for s in test_corpus]\n",
        "test_labels = [[0, 0, 0, 1, 0, 1]]\n",
        "\n",
        "# Create a test loader\n",
        "test_data = list(zip(test_sentences, test_labels))\n",
        "batch_size = 1\n",
        "shuffle = False\n",
        "window_size = 2\n",
        "collate_fn = partial(custom_collate_fn, window_size=2, word_to_ix=word_to_ix)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,\n",
        "                                           batch_size=1,\n",
        "                                           shuffle=False,\n",
        "                                           collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "_KQoMO1_VhY3"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for test_instance, labels, _ in test_loader:\n",
        "  outputs = model.forward(test_instance)\n",
        "  print(labels)\n",
        "  print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtZY5iTbVi_A",
        "outputId": "758dcdbd-c806-460c-ee43-57c8ada7ca70"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0, 1, 0, 1]])\n",
            "tensor([[0.0755, 0.0448, 0.0708, 0.0620, 0.0743, 0.0561]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    }
  ]
}